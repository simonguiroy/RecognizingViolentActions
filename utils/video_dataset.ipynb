{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import time\n",
    "from preprocessing import preprocess_rgb, preprocess_flow\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 csv_file, \n",
    "                 root_dir,\n",
    "                 stream='rgb',\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.csv_data = genfromtxt(csv_file, delimiter=',', dtype=str)\n",
    "        self.root_dir = root_dir\n",
    "        self.stream = stream\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv_data.shape[0]\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.csv_data[idx][0]\n",
    "        label = self.csv_data[idx][1]\n",
    "        sample = {'videos': {'video_rgb': torch.zeros(224, 224), 'video_flow': torch.randn(224, 224)}, 'label': label}\n",
    "        \n",
    "        if self.stream == 'rgb' or self.stream == 'joint':\n",
    "            sample['videos']['video_rgb'] = torch.from_numpy(preprocess_rgb(self.root_dir + video_id + '.mp4'))\n",
    "        if self.stream == 'flow' or self.stream == 'joint':\n",
    "            sample['videos']['video_flow'] = preprocess_flow(self.root_dir + video_id + '.mp4')\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset=VideoDataset(\"./data/video_dataset.csv\", \"./data/\", stream='rgb'),\n",
    "                    batch_size=1,\n",
    "                    shuffle=False,\n",
    "                    num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "torch.Size([1, 1, 150, 224, 224, 3])\n",
      "---------------------------------\n",
      "torch.Size([1, 1, 150, 224, 224, 3])\n",
      "---------------------------------\n",
      "torch.Size([1, 1, 150, 224, 224, 3])\n",
      "---------------------------------\n",
      "torch.Size([1, 1, 150, 224, 224, 3])\n",
      "---------------------------------\n",
      "torch.Size([1, 1, 150, 224, 224, 3])\n",
      "---------------------------------\n",
      "torch.Size([1, 1, 134, 224, 224, 3])\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(loader):\n",
    "    print ('---------------------------------')\n",
    "    print (batch['videos']['video_rgb'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid2 = batch['videos']['video_rgb'].view(134,224,224,3).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 224, 224, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (3,224,224) into shape (134,224,224)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bdcc67811177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mframe2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mframes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (3,224,224) into shape (134,224,224)"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from array2gif import write_gif\n",
    "\n",
    "# This script runs a gym environment and creates a video from the numpy array.\n",
    "# Also a good example of how to create a video from a numpy array, using OpenCV\n",
    "\n",
    "frame = vid2[0,:,:,:]\n",
    "\n",
    "\n",
    "N_steps = 134\n",
    "#frames = np.zeros(frame.shape[0], frame.shape[1], frame.shape[2], N_steps)\n",
    "#frames[:,:,:,0] = frame\n",
    "frames = vid2\n",
    "\n",
    "# frame dimensions\n",
    "# 400 x 600 x 3\n",
    "\n",
    "#frames = np.ndarray(shape=(frame.shape[0], frame.shape[1], frame.shape[2], N_steps), dtype=int16)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
    "out = cv2.VideoWriter('simple_output.avi',fourcc, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "#out = cv2.VideoWriter('simple_output.avi', -1, 20.0, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "#writer = cvCreateVideoWriter('simple_output.avi', -1, 20.0, (frame.shape[0], frame.shape[1]), is_color=1)\n",
    "\n",
    "i = 1\n",
    "for i in range(N_steps - 1):\n",
    "    frame = vid2[i,:,:,:]\n",
    "\n",
    "    frame2 = np.reshape( frame, (frame.shape[2], frame.shape[0], frame.shape[1]) ) \n",
    "    frames[:,:,:, i] = frame2\n",
    "    frames.append(frame2)\n",
    "    frames.append(frame[:,:,0])\n",
    "    out.write(frame)\n",
    "    #cvWriteFrame(out, frame)\n",
    "    i+=1\n",
    "\n",
    "print (\"Environment closed\")\n",
    "out.release()\n",
    "#im = Image.fromarray(frame)\n",
    "#im.save(\"frame.jpeg\")\n",
    "write_gif(frames, 'rgb.gif', fps=24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(vid2[100,:,:,1].all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
